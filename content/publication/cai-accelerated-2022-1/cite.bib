@misc{cai_accelerated_2022-1,
 abstract = {We study monotone inclusions and monotone variational inequalities, as well as their generalizations to non-monotone settings. We first show that the Extra Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu [2021] for unconstrained convex-concave min-max optimization, can be applied to solve the more general problem of Lipschitz monotone inclusion. More specifically, we prove that the EAG solves Lipschitz monotone inclusion problems with an accelerated convergence rate of \$O(\frac\1\\T\)\$, which is optimal among all first-order methods [Diakonikolas, 2020, Yoon and Ryu, 2021]. Our second result is an accelerated forward-backward splitting algorithm (AS), which not only achieves the accelerated \$O(\frac\1\\T\)\$ convergence rate for all monotone inclusion problems, but also exhibits the same accelerated rate for a family of general (non-monotone) inclusion problems that concern negative comonotone operators. As a special case of our second result, AS enjoys the \$O(\frac\1\\T\)\$ convergence rate for solving a non-trivial class of nonconvex-nonconcave min-max optimization problems. Our analyses are based on simple potential function arguments, which might be useful for analysing other accelerated algorithms.},
 author = {Cai, Yang and Oikonomou, Argyris and Zheng, Weiqiang},
 doi = {10.48550/arXiv.2206.05248},
 file = {arXiv Fulltext PDF:files/697/Cai ç­‰ - 2022 - Accelerated Algorithms for Monotone Inclusion and .pdf:application/pdf;arXiv.org Snapshot:files/700/2206.html:text/html},
 keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Optimization and Control},
 month = {August},
 note = {arXiv:2206.05248 [cs, math]},
 publisher = {arXiv},
 title = {Accelerated Algorithms for Monotone Inclusion and Constrained Nonconvex-Nonconcave Min-Max Optimization},
 url = {http://arxiv.org/abs/2206.05248},
 urldate = {2023-11-26},
 year = {2022}
}
