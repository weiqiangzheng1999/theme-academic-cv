---
title: Accelerated Algorithms for Monotone Inclusion and Constrained Nonconvex-Nonconcave
  Min-Max Optimization
authors:
- Yang Cai
- Argyris Oikonomou
- Weiqiang Zheng
date: '2022-08-01'
publishDate: '2023-11-26T02:45:28.720441Z'
publication_types:
- manuscript
publication: 'Working Paper. NeurIPS 2022 OPT Workshop.'
doi: 10.48550/arXiv.2206.05248
abstract: We study monotone inclusions and monotone variational inequalities, as well
  as their generalizations to non-monotone settings. We first show that the Extra
  Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu [2021] for
  unconstrained convex-concave min-max optimization, can be applied to solve the more
  general problem of Lipschitz monotone inclusion. More specifically, we prove that
  the EAG solves Lipschitz monotone inclusion problems with an accelerated convergence
  rate of {{< math >}}$ \frac{1}{T} ${{< /math >}}, which is optimal among all first-order methods [Diakonikolas,
  2020, Yoon and Ryu, 2021]. Our second result is an accelerated forward-backward
  splitting algorithm (AS), which not only achieves the accelerated $O(frac1T)$ convergence
  rate for all monotone inclusion problems, but also exhibits the same accelerated
  rate for a family of general (non-monotone) inclusion problems that concern negative
  comonotone operators. As a special case of our second result, AS enjoys the {{< math >}}$ \frac{1}{T} ${{< /math >}}
  convergence rate for solving a non-trivial class of nonconvex-nonconcave min-max
  optimization problems. Our analyses are based on simple potential function arguments,
  which might be useful for analysing other accelerated algorithms.
tags:
- Computer Science - Data Structures and Algorithms
- Computer Science - Machine Learning
- Mathematics - Optimization and Control
links:
- name: arXiv
  url: http://arxiv.org/abs/2206.05248
---
